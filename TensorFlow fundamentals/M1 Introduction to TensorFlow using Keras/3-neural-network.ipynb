{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import tensorflow as tf"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to classify an input image into one of the 10 classes of clothing, so we will define our neural network to take as input a matrix of shape (28, 28) and output a vector of size 10, where the index of the largest value in the output corresponds to the integer label for the class of clothing in the image. For example, if we use an image of an ankle boot as input, we might get an output vector $y'$ like this:\n",
        "\n",
        "$$\n",
        "y' =\n",
        "\\begin{bmatrix}\n",
        "  0.0000 \\\\\n",
        "  5.3003 \\\\\n",
        "  2.1616 \\\\\n",
        "  1.9145 \\\\\n",
        "  0.0000 \\\\\n",
        "  5.1698 \\\\\n",
        "  0.0000 \\\\\n",
        "  2.2152 \\\\\n",
        "  0.0000 \\\\\n",
        "  7.0417 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In this particular example, the largest value appears at index 9 (counting from zero) &mdash; and as we showed in the previous module, index 9 corresponds to the \"Ankle Boot\" category. So this indicates that our neural network correctly classified the image of an ankle boot.\n",
        "\n",
        "Here's a visualization of the structure of the neural network we chose for this scenario:\n",
        "\n",
        "![Diagram of Fashion MNIST classification neural network](notebooks/images/1-fashion-nn.png)\n",
        "\n",
        "Because each image has 28 &times; 28 = 784 pixels, we need 784 nodes in the input layer (one for each pixel value). We decided to add one hidden layer with 20 nodes and a ReLU (rectified linear unit) activation function. We want the output of our network to be a vector of size 10, therefore our output layer needs to have 10 nodes.\n",
        "\n",
        "Here's the Keras code that defines this neural network:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.sequence = tf.keras.Sequential([\n",
        "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "      tf.keras.layers.Dense(20, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "\n",
        "  def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "    y_prime = self.sequence(x)\n",
        "    return y_prime"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) layer turns our input matrix of shape (28, 28) into a vector of size 784. The [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers are also known as \"fully connected\" or \"linear\" layers because they connect all nodes from the previous layer with each of their own nodes using a linear function. Notice that they specify \"ReLU\" as the activation &mdash; that's because we want the results of the linear mathematical operation to get passed as input to a \"Rectified Linear Unit\" function, which adds non-linearity to the calculations. \n",
        "\n",
        "It's important to have non-linear activation functions (like the ReLU function) between linear layers, because otherwise a sequence of linear layers would be mathematically equivalent to just one layer. These activation functions give our network more expressive power, allowing it to approximate non-linear relationships between data.\n",
        "\n",
        "The [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) class combines all the other layers. Lastly, we define the `call` method, which supplies a tensor `x` as input to the `sequence` of layers and produces the `y_prime` vector as a result.\n",
        "\n",
        "We can print a description of our model using the `summary` method:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork()\n",
        "model.build((1, 28, 28))\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"neural_network\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nsequential (Sequential)      (None, 10)                15910     \n=================================================================\nTotal params: 15,910\nTrainable params: 15,910\nNon-trainable params: 0\n_________________________________________________________________\n"
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is all the code needed to define our neural network. Now that we have a neural network and some data, it's time to train the neural network using that data. "
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
    },
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "kernelspec": {
      "display_name": "azureml_py38_PT_and_TF",
      "language": "python",
      "name": "azureml_py38_PT_and_TF"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}